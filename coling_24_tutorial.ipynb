{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Paper Hypothesis Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install sentence-transformers\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's download data and load the model from Huggingface\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"eaglew/COLING2024\", local_dir=cwd, filename=\"train.json\")\n",
    "hf_hub_download(repo_id=\"eaglew/COLING2024\", local_dir=cwd, filename=\"test.json\")\n",
    "hf_hub_download(repo_id=\"eaglew/COLING2024\", local_dir=cwd, filename=\"BEST.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will have 2 steps:\n",
    "1. Retrieve inspirations (related entity) based on cosine similarity\n",
    "2. Generate new hypothesis based on the inspirations  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.modeling_outputs import ModelOutput, BaseModelOutputWithPastAndCrossAttentions, BaseModelOutput, Seq2SeqModelOutput, Seq2SeqLMOutput\n",
    "from transformers.utils import logging\n",
    "\n",
    "from transformers.models.t5.modeling_t5 import (\n",
    "    T5Attention,\n",
    "    T5LayerSelfAttention,\n",
    "    T5LayerCrossAttention, \n",
    "    T5Block,\n",
    "    T5Stack,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config,\n",
    "    T5LayerNorm,\n",
    ")\n",
    "import copy\n",
    "import warnings\n",
    "import math\n",
    "import pickle\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ContrastiveSeq2SeqModelOutput(ModelOutput):\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    retrieve_last_hidden_states: Optional[torch.FloatTensor] = None\n",
    "\n",
    "@dataclass\n",
    "class ContrastiveSeq2SeqLMOutput(ModelOutput):\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    mlm_loss: Optional[torch.FloatTensor] = None\n",
    "    cl_loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "\n",
    "class ContrastiveHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inner_dim: int,\n",
    "        pooler_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "        self.out_proj = nn.Linear(inner_dim, 1)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, masks: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.out_proj(hidden_states)\n",
    "        hidden_states = self.avg_pool(hidden_states, masks)\n",
    "        hidden_states = torch.sigmoid(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "    def avg_pool(self, hidden_states, mask):\n",
    "        length = torch.sum(mask, 1, keepdim=True).float()\n",
    "        mask = mask.unsqueeze(2)\n",
    "        hidden = hidden_states.masked_fill(mask == 0, 0.0)\n",
    "        avg_hidden = torch.sum(hidden, 1) / length\n",
    "\n",
    "        return avg_hidden\n",
    "\n",
    "__HEAD_MASK_WARNING_MSG = \"\"\"\n",
    "The input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n",
    "`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\n",
    "If you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\n",
    "num_heads)`.\n",
    "\"\"\"\n",
    "\n",
    "class ContrastiveT5ForConditionalGeneration(T5ForConditionalGeneration):\n",
    "\n",
    "    def __init__(self, config: T5Config):\n",
    "        super().__init__(config)\n",
    "        self.model_dim = config.d_model\n",
    "\n",
    "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        encoder_config = copy.deepcopy(config)\n",
    "        encoder_config.is_decoder = False\n",
    "        encoder_config.use_cache = False\n",
    "        encoder_config.is_encoder_decoder = False\n",
    "        self.encoder = T5Stack(encoder_config, self.shared)\n",
    "\n",
    "        decoder_config = copy.deepcopy(config)\n",
    "        decoder_config.is_decoder = True\n",
    "        decoder_config.is_encoder_decoder = False\n",
    "        decoder_config.num_layers = config.num_decoder_layers\n",
    "        self.decoder = T5Stack(decoder_config, self.shared)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        self.contrastive_head = ContrastiveHead(config.d_model, config.dropout_rate)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "\n",
    "        neg_ids: Optional[torch.LongTensor] = None,\n",
    "        neg_num_total: Optional[int]=1,\n",
    "        valid: Optional[bool] = False,\n",
    "        ppl: Optional[bool] = False,\n",
    "\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
    "        if head_mask is not None and decoder_head_mask is None:\n",
    "            if self.config.num_layers == self.config.num_decoder_layers:\n",
    "                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
    "                decoder_head_mask = head_mask\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        if encoder_outputs is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            # get decoder inputs from shifting lm labels to the right\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.encoder.first_device)\n",
    "            self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
    "            sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
    "\n",
    "        if self.config.tie_word_embeddings:\n",
    "            # Rescale output before projecting on vocab\n",
    "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "            sequence_output = sequence_output * (self.model_dim**-0.5)\n",
    "\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        contrastive_loss = None\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if ppl:\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=-100, reduction='sum')\n",
    "                ppl_list = []\n",
    "                for k in range(labels.size(0)):\n",
    "                    masked_lm_loss = loss_fct(lm_logits[k,:,:], labels[k,:])\n",
    "                    # prob = torch.exp(-masked_lm_loss)\n",
    "                    prob = -masked_lm_loss\n",
    "                    ppl_list.append(prob.item())\n",
    "                return ppl_list\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            \n",
    "            if not valid:\n",
    "                contrastive_loss = self.contrastive(\n",
    "                    decoder_outputs,\n",
    "                    labels,\n",
    "                    attention_mask,\n",
    "                    neg_ids,\n",
    "                    neg_num_total,\n",
    "                    encoder_outputs,\n",
    "                    decoder_attention_mask,\n",
    "                    past_key_values,\n",
    "                    decoder_inputs_embeds,\n",
    "                    decoder_head_mask,\n",
    "                    output_attentions,\n",
    "                    cross_attn_head_mask,\n",
    "                    output_hidden_states,\n",
    "                    return_dict,\n",
    "                    use_cache)\n",
    "                loss = masked_lm_loss + contrastive_loss\n",
    "            else:\n",
    "                loss = masked_lm_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
    "            return ((loss,masked_lm_loss,contrastive_loss,) + output) if loss is not None else output\n",
    "\n",
    "        return ContrastiveSeq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            mlm_loss=masked_lm_loss,\n",
    "            cl_loss=contrastive_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def contrastive(\n",
    "        self,\n",
    "        decoder_outputs,\n",
    "        labels,\n",
    "        attention_mask,\n",
    "        neg_ids,\n",
    "        expand_size,\n",
    "        encoder_outputs,\n",
    "        decoder_attention_mask,\n",
    "        past_key_values,\n",
    "        decoder_inputs_embeds,\n",
    "        decoder_head_mask,\n",
    "        output_attentions,\n",
    "        cross_attn_head_mask,\n",
    "        output_hidden_states,\n",
    "        return_dict,\n",
    "        use_cache\n",
    "    ):\n",
    "        pos_label_mask = labels != -100\n",
    "        pos_emb = self.contrastive_head(decoder_outputs.last_hidden_state, pos_label_mask)\n",
    "\n",
    "\n",
    "        decoder_input_ids = self._shift_right(neg_ids)\n",
    "        bs = labels.size(0)\n",
    "\n",
    "        expanded_return_idx = (\n",
    "            torch.arange(attention_mask.shape[0]).view(-1, 1).repeat(1, expand_size).view(-1).to(attention_mask.device)\n",
    "        )\n",
    "\n",
    "        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "            0, expanded_return_idx.to(encoder_outputs.last_hidden_state.device)\n",
    "        )\n",
    "\n",
    "        attention_mask = attention_mask.index_select(0, expanded_return_idx).to(attention_mask.device)\n",
    "\n",
    "\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "\n",
    "        neg_label_mask = neg_ids != self.config.pad_token_id\n",
    "        neg_emb = self.contrastive_head(decoder_outputs.last_hidden_state, neg_label_mask).view(bs, expand_size)\n",
    "\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        all_logit = torch.cat([pos_emb,neg_emb], dim=1)\n",
    "        l = torch.zeros([bs], dtype=torch.long, device=neg_emb.device)\n",
    "        cl_loss = loss_fct(all_logit, l)\n",
    "        return cl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will try to encode all instances in the training set with sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this example, we're going to use 'all-MiniLM-L6-v2' to encode all sentences\n",
    "\n",
    "\n",
    "model_sent = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "context2cands = defaultdict(dict)\n",
    "context2c = defaultdict(set)\n",
    "context2ids = {}\n",
    "contexts = []\n",
    "\n",
    "in_name = 'train.json'\n",
    "with open(in_name, 'r') as file_j:\n",
    "    for idxn, line in tqdm(enumerate(file_j), \"Encoding\"):\n",
    "        pdf_dict = json.loads(line)\n",
    "        input_, context_ = pdf_dict['input'].split('| context: ')\n",
    "        type_ = input_.split()[-1]\n",
    "        context = input_ + ' context: ' + context_\n",
    "        context2ids[context] = pdf_dict['id']\n",
    "        context2cands[context][pdf_dict['output']] = type_\n",
    "        context2c[context].add(context_)\n",
    "        contexts.append(context)\n",
    "print('finish loading')\n",
    "context_embeddings = model_sent.encode(contexts, batch_size=10, convert_to_tensor=True,show_progress_bar=True)\n",
    "\n",
    "# this is the function that help us find ideas proposed for related problems in the training set\n",
    "def get_retrieve(current_context, contexts, context_embeddings, type_):\n",
    "    c_embedding = model_sent.encode([current_context], convert_to_tensor=True)\n",
    "    hits = util.semantic_search(c_embedding, context_embeddings, top_k=100)\n",
    "    retrieve = {}\n",
    "    count = 0\n",
    "    for i in range(20):\n",
    "        cur_id = hits[0][i]['corpus_id']\n",
    "        context = contexts[cur_id]\n",
    "        retrieve.update(context2cands[context])\n",
    "    \n",
    "    entities = []\n",
    "    for entity in retrieve:\n",
    "        if retrieve[entity] == type_:\n",
    "            entities.append(entity)\n",
    "\n",
    "        \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's load the model from Huggingface \n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "PATH = \"BEST.pth\"\n",
    "device = torch.device('cpu')\n",
    "model = ContrastiveT5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "filename = \"test.json\"\n",
    "test_dataset = []\n",
    "with open(filename, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        test_dataset.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's randomly select some test set from the test set and try to generate hypothesis based on the background context and seed term\n",
    "\n",
    "import random\n",
    "cur_data = random.choice(test_dataset)\n",
    "\n",
    "input_ = cur_data[\"input\"]\n",
    "target = cur_data[\"rel_sent\"].lower()\n",
    "ids = cur_data[\"id\"]\n",
    "year = cur_data[\"year\"]\n",
    "input_, context_ = cur_data['input'].split('| context: ')\n",
    "type_ = input_.split()[-1]\n",
    "tmp = copy.deepcopy(pdf_dict)\n",
    "current_context = input_ + ' context: ' + context_\n",
    "retrieved_entity = get_retrieve(current_context, contexts, context_embeddings, type_)\n",
    "new_input  = input_ + ' | retrieve: ' + ', '.join(retrieved_entity) + ' | context: ' + context_\n",
    "print('seed term:', input_)\n",
    "print('background context:', context_)\n",
    "print('semantic related entities:', ', '.join(retrieved_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id = tokenizer(new_input, truncation=True, max_length=512).input_ids\n",
    "input_ids = torch.LongTensor([source_id])\n",
    "output = model.generate(input_ids = source_id, \n",
    "                max_length=100,\n",
    "                repetition_penalty=1.5,\n",
    "                num_beams=10)\n",
    "predictions = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
